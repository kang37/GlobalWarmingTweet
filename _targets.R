# This is a targets script generated by tar_script() and edited after that. This script makes a folder called "_targets", with meta data and files of targets inside it. 
library(targets)

# Define custom functions and other global objects.
get_event <- 
  function(csv_x, quant_x) {
    csv_x %>% 
      # Number of tweets for each day. 
      group_by(year, month, date, annual_user) %>% 
      summarise(tw_num = n()) %>% 
      ungroup() %>% 
      mutate(tw_int = tw_num / annual_user) %>% 
      # Highlight the high-tweet-number date: compared with annual 90% quantile tweet number intensity. 
      group_by(year) %>% 
      mutate(quant = quantile(tw_int, quant_x)) %>% 
      ungroup() %>% 
      filter(tw_int >= quant) %>% 
      arrange(year, date) %>% 
      mutate(
        date_backward = lag(date, 1), 
        date_forward = lead(date, 1), 
        date_diff_pre = date - date_backward, 
        date_diff_after = date_forward - date, 
        continue_condition = c(
          date_diff_pre == 1 | date_diff_after == 1 | 
            is.na(date_diff_pre) | is.na(date_diff_after)
        ), 
        # Make groups: every period of continue date is considered as one group. 
        change_condition = 
          c(date_diff_pre != 1 | is.na(date_diff_pre))
      ) %>% 
      mutate(grp = cumsum(change_condition)) %>% 
      group_by(grp) %>% 
      mutate(
        row_num = row_number(), 
        max_row_num = max(row_num)
      ) %>% 
      ungroup() %>% 
      # Keep days of continue date of high-tweet-number >= 3 days. 
      filter(max_row_num >= 3) %>% 
      return()
  }

# Set target-specific options such as packages:
tar_option_set(packages = c(
  "dplyr", "tidyr", "purrr", "quanteda", "quanteda.textstats", 
  "quanteda.textplots", "stopwords", "LSX", "ggplot2", "ggraph", "patchwork", 
  "tidygraph", "lubridate"
)) 

# List of target objects.
list(
  tar_target(
    user_num, 
    tibble(
      year = 2012:2022, 
      # Unit of total Twitter user number: thousand people. 
      annual_user = c(
        20032.101, 22297.45, 27864.903, 33680.175, 34936.55, 39471.809, 
        47277.377, 48976.785, 53359.758, 57981.924, 56600.991
      )
    ) %>% 
      # Turn unit into people. 
      mutate(annual_user = annual_user * 1000)
  ), 
  # Get tweet data based on tweet data in *.csv files
  tar_target(
    csv, 
    lapply(
      # Raw data from 2012 to 2021 are all complete year data; while data for 2022 is not complete. 
      list.files("data_raw/tweet_csv", full.names = TRUE), 
      function(x) {
        data.table::fread(x) %>% 
          mutate(
            id = as.character(id), 
            # Turn UTC to JST time. 
            created_at = as_date(with_tz(created_at, "Asia/Tokyo")), 
            author_id = as.character(author_id), 
            retweeted_user_id = as.character(retweeted_user_id), 
            year = year(created_at), 
            month = month(created_at)
          ) %>% 
          tibble() %>% 
          select(
            id, date = created_at, year, month, 
            author_id, author_username = author.username, 
            author_name = author.name, 
            author_description = author.description, 
            followers_count = author.public_metrics.followers_count, 
            text, retweet_count = public_metrics.retweet_count, 
            retweeted_user_id
          )
      }
    ) %>% 
      do.call(rbind, .) %>% 
      left_join(user_num, by = "year")
  ), 
  # Tweet number change plot. 
  tar_target(
    tw_num_plot, 
    csv %>% 
      mutate(retweet = case_when(
      is.na(retweeted_user_id) ~ "Non-retweet", 
      TRUE ~ "Retweet"
    )) %>% 
      group_by(date, retweet) %>% 
      summarise(tw_num = n()) %>% 
      ungroup() %>%
      mutate(retweet = factor(retweet, levels = c("Retweet", "Non-retweet"))) %>% 
      ggplot() + 
      geom_col(aes(date, tw_num, fill = retweet)) + 
      labs(x = "Date", y = "Tweet number") + 
      scale_fill_discrete(name = "Tweet") + 
      theme(legend.justification = c(0, 1))
  ), 
  # Change of daily tweet number per 10 thousand users. 
  tar_target(
    general_plot_dt, 
    csv %>% 
      group_by(date, annual_user) %>% 
      summarise(
        tw_num = n(), 
        tw_int = tw_num / annual_user * 100000
      ) %>% 
      ungroup()
  ), 
  # The event-identification: continue date with high-tweet-number. 
  tar_target(
    tw_high_90, 
    get_event(csv_x = csv, quant_x = 0.9)
  ), 
  tar_target(
    tw_high_85, 
    get_event(csv_x = csv, quant_x = 0.85)
  ), 
  # Event period extraction.
  tar_target(
    event_90, 
    tw_high_90 %>% 
      mutate(period = case_when(
        row_num == 1 ~ "start_day", 
        row_num == max_row_num ~ "end_day"
      )) %>% 
      select(grp, date, period) %>% 
      filter(!is.na(period)) %>% 
      pivot_wider(id_cols = grp, names_from = period, values_from = date) %>% 
      mutate(event_id = row_number(), .before = 1)
  ), 
  # Raw data for each event. 
  tar_target(
    csv_raw, 
    map2(
      event_90$start_day, event_90$end_day, 
      function(start_x, end_x) {
        filter(csv, date >= start_x, date <= end_x)
      }
    ) %>% 
      set_names(event_90$event_id)
  ), 
  # Further process the event raw data. 
  tar_target(
    csv_event, 
    lapply(
      csv_raw,
      function(x) {
        select(
          x, id, date, author_id, retweeted_user_id, author_username, text
        ) %>% 
          distinct() %>% 
          mutate(
            author_id = as.character(author_id), 
            retweeted_user_id = as.character(retweeted_user_id)
          ) %>% 
          # Bug: Here, "no retweeted, no impact" is an arbitrary assumption. 
          filter(!is.na(retweeted_user_id))
      }
    )
  ), 
  tar_target(
    author_attr, 
    lapply(
      csv_raw, 
      function(x) select(
        x, author_id, author_username, author_name, 
        followers_count, author_description
      )
    ) %>% 
      do.call(rbind, .) %>% 
      distinct()
  ), 
  # Calculate centrality of the nodes. 
  tar_target(
    graph_cen, 
    lapply(
      names(csv_event), 
      function(x) {
        select(csv_event[[x]], author_id, retweeted_user_id) %>% 
          rename(from = retweeted_user_id, to = author_id) %>% 
          as_tbl_graph() %>% 
          activate(nodes) %>% 
          mutate(
            cen_degree = centrality_degree(), 
            cen_between = centrality_betweenness()
          ) %>% 
          activate(nodes) %>% 
          data.frame() %>% 
          tibble() %>% 
          arrange(-cen_degree) %>% 
          mutate(
            cen_degree_mvp = c(rep(TRUE, 10), rep(FALSE, nrow(.) - 10))
          ) %>% 
          arrange(-cen_between) %>% 
          mutate(cen_between_mvp = c(rep(TRUE, 10), rep(FALSE, nrow(.) - 10))) %>% 
          mutate(
            mvp_grp = case_when(
              cen_degree_mvp + cen_between_mvp == 0 ~ "no_mvp", 
              cen_degree_mvp + cen_between_mvp == 2 ~ "multi_mvp", 
              cen_degree_mvp + cen_between_mvp == 1 & cen_degree_mvp ~ "degree_mvp", 
              cen_degree_mvp + cen_between_mvp == 1 & cen_between_mvp ~ "between_mvp"
            ), 
            label = case_when(
              cen_degree_mvp + cen_between_mvp > 0 ~ name, 
              TRUE ~ NA_character_
            )
          ) %>% 
          group_by(mvp_grp) %>% 
          mutate(mvp_id = row_number()) %>% 
          ungroup() %>% 
          mutate(mvp_id = case_when(
            mvp_grp == "no_mvp" ~ NA_character_,
            mvp_grp == "multi_mvp" ~ paste0("M", mvp_id),
            mvp_grp == "degree_mvp" ~ paste0("D", mvp_id),
            mvp_grp == "between_mvp" ~ paste0("B", mvp_id)
          )) %>% 
          left_join(
            distinct(select(author_attr, author_id, author_username)), 
            by = c("name" = "author_id")
          ) %>% 
          # Some times one author_id has more than one username, so keep the first here. 
          group_by(name) %>% 
          mutate(row_num = row_number()) %>% 
          ungroup() %>% 
          filter(row_num == 1) %>% 
          select(-row_num)
      }
    ) %>% 
      setNames(names(csv_event))
  ), 
  # Users with high degree centrality.
  tar_target(
    top_degree, 
    lapply(
      graph_cen, 
      function(x) {
        x %>% 
          arrange(-cen_degree) %>% 
          select(userid = name, cen_degree) %>% 
          head(10) %>% 
          left_join(author_attr, by = c("userid" = "author_id")) %>% 
          # Since one user might have a changing follower count etc., so let's combine the potential multiple rows of the fields into one row. 
          group_by(userid, cen_degree) %>% 
          summarise(
            min_followers_count = min(followers_count), 
            max_followers_count = max(followers_count), 
            author_username = 
              paste(unique(author_username), collapse = "----"), 
            author_name = 
              paste(unique(author_name), collapse = "----"), 
            author_description = 
              paste(unique(author_description), collapse = "----"), 
            .groups = "drop"
          )
      }
    )
  ), 
  # Users with high between centrality. 
  tar_target(
    top_between, 
    lapply(
      graph_cen, 
      function(x) {
        x %>% 
          arrange(-cen_between) %>% 
          select(userid = name, cen_between
          ) %>% 
          head(10) %>% 
          left_join(author_attr, by = c("userid" = "author_id")) %>% 
          # Since one user might have a changing follower count etc., so let's combine the potential multiple rows of the fields into one row. 
          group_by(userid) %>% 
          summarise(
            min_followers_count = min(followers_count), 
            max_followers_count = max(followers_count), 
            author_username = 
              paste(unique(author_username), collapse = "----"), 
            author_name = 
              paste(unique(author_name), collapse = "----"), 
            author_description = 
              paste(unique(author_description), collapse = "----"), 
            .groups = "drop"
          )
      }
    )
  ), 
  # Data for network plots. 
  tar_target(
    graph, 
    lapply(
      names(csv_event), 
      function(x) {
        filter(
          csv_event[[x]], 
          retweeted_user_id %in% 
            c(head(arrange(graph_cen[[x]], -cen_degree), 100)$name, 
              head(arrange(graph_cen[[x]], -cen_between), 100)$name)
        ) %>% 
          filter(
            author_id %in% 
              c(head(arrange(graph_cen[[x]], -cen_degree), 100)$name, 
                head(arrange(graph_cen[[x]], -cen_between), 100)$name)
          ) %>% 
          select(author_id, retweeted_user_id) %>% 
          rename(from = retweeted_user_id, to = author_id) %>% 
          as_tbl_graph() %>% 
          activate(nodes) %>% 
          left_join(graph_cen[[x]], by = "name") %>% 
          # Community detection. 
          # Bug: Need to figure out the best community-detection function for this case. Besides, this detection is based on filtered nodes, rather than raw data - if raw data is used, there will be too many communities. 
          mutate(community = as.factor(group_components())) %>% 
          mutate(author_username = case_when(
            mvp_grp == "no_mvp" ~ NA_character_, 
            mvp_grp != "no_mvp" & is.na(author_username) ~ "", 
            TRUE ~ author_username
          ))
      }
    ) %>% 
      setNames(names(csv_event))
  ), 
  # Network plots. 
  tar_target(
    net_plot_cen, 
    lapply(
      graph, 
      function(x) {
        ggraph(x, layout = 'kk') + 
          geom_edge_link(alpha = 0.3) + 
          geom_node_point(aes(size = cen_degree, col = mvp_grp), alpha = 0.9) + 
          geom_node_label(aes(label = author_username), size = 2, alpha = 0.5)
      }
    )
  ), 
  tar_target(
    net_plot_comm, 
    lapply(
      graph, 
      function(x) {
        ggraph(x, layout = 'kk') + 
          geom_edge_link(alpha = 0.3) + 
          geom_node_point(aes(size = cen_degree, col = community), alpha = 0.9) + 
          geom_node_label(aes(label = author_username), size = 2, alpha = 0.5)
      }
    )
  ), 
  # Text analysis. 
  # Make corpus. 
  tar_target(
    corp, 
    lapply(
      csv_raw, 
      function(x) corpus(x, text_field = "text")
    )
  ), 
  # User-defined dictionary. 
  tar_target(
    dict, 
    list(
      "地球 温暖 化", 
      "国立 環境 研究所", 
      "環境 研究所",
      "環境 研", 
      "江守 正多", 
      "江守 正多 先生", 
      "グレタ トゥーンベリ", 
      "地球 環境 研究 センター", 
      "再 稼働", 
      "強制 移住 防止 策", 
      "温暖 化", 
      "京都 議定書", 
      "松岡 修造", 
      "温室 効果 ガス", 
      "寒冷 化", 
      "砂漠 化", 
      "CO2", 
      "CO２", 
      "オゾン 層", 
      "可能 性", 
      "排出 量", 
      "議定 書", 
      "熱 中 症", 
      "科学 者", 
      "宇宙 ゴミ", 
      "数 年", 
      "パリ 協定", 
      "東京 タワー", 
      "小泉 進次郎", 
      "レジ 袋",
      "私 たち",
      "子供 たち",
      "生態 系",
      "花粉 症",
      "令 和",
      "化石 燃料",
      "気候 変動",
      "異常 気象",
      "気象 異常",
      "森林 伐採",
      "高齢 化",
      "子供 ごろ", 
      "子供 頃",
      "クマゼミ",
      "政治 家",
      "先進 国",
      "途上 国",
      "気温 上昇",
      "海面 上昇",
      "火力 発電",
      "氷 期",
      "具体 的",
      "人為 的",
      "十 年",
      "軍事 費",
      "人 たち",
      "自然 災害",
      "感染 症",
      "世界 的",
      "永久 凍土",
      "有料 化", 
      "一 番",
      "電気 代",
      "バイデン 氏",
      "COP",
      "科学 的",
      "陰謀 論", 
      "今 年", 
      "世界 の 財界",
      "今 月",
      "今 週",
      "今 日", 
      "麻生 氏", 
      "麻生 太郎", 
      "気候 変動"
    ) %>%  
      unique() %>% 
      setNames(1:length(.)) %>% 
      dictionary()
  ), 
  # Stop words. 
  tar_target(
    stopword, 
    c(stopwords("ja", source = "marimo"), 
      "amp", "ます", "です", "こと", "って", "てい", "という", "んで", "ので", 
      "なく", "など", "なる", "せん", "しま", "とか", "しょう", "ろう", "けど", 
      "さん", "あっ", "られる", "ぜひ", "てる")
  ),
  # Tokenization. 
  tar_target(
    tok, 
    lapply(
      corp, 
      function(x) {
        tokens(
          x, 
          remove_symbols = TRUE, 
          remove_numbers = TRUE, 
          remove_url = TRUE, 
          remove_separators = TRUE, 
          remove_punct = TRUE
        ) %>% 
          # Bug: Should delete text with just a few words? 
          tokens_compound(pattern = dict, concatenator = "") %>% 
          # Keep tokens in Japanese. 
          # Bug: Need to keep the words in other language? 
          tokens_select(
            pattern = "^[ぁ-んァ-ヶー一-龠]+$", valuetype = "regex", padding = TRUE
          ) %>% 
          tokens_keep(min_nchar = 2) %>% 
          tokens_remove(pattern = stopword)
      }
    )
  ), 
  # Document-term matrix. 
  tar_target(
    df_matrix, 
    lapply(tok, dfm)
  )
)
