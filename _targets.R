# This is a targets script generated by tar_script() and edited after that. This script makes a folder called "_targets", with meta data and files of targets inside it. 
library(targets)

# Set target-specific options such as packages:
tar_option_set(packages = c(
  "dplyr", "tidyr", "purrr", "quanteda", "quanteda.textstats", 
  "quanteda.textplots", "stopwords", "LSX", "ggplot2", "ggraph", "patchwork", 
  "tidygraph", "lubridate", "topicmodels", "tidytext"
)) 

# List of target objects.
list(
  # Get tweet data based on tweet data in *.csv files
  tar_target(
    csv, 
    lapply(
      # Raw data from 2018 to 2021 are all complete year data; while data for 2022 is not complete. 
      list.files("data_raw/tweet_csv", full.names = TRUE), 
      function(x) {
        data.table::fread(x) %>% 
          mutate(
            id = as.character(id), 
            # Turn UTC to JST time. 
            created_at = as_date(with_tz(created_at, "Asia/Tokyo")), 
            author_id = as.character(author.id), 
            year = year(created_at), 
            month = month(created_at)
          ) %>% 
          tibble() %>% 
          select(
            id, date = created_at, year, month, 
            author_id, author_username = author.username, 
            author_name = author.name, 
            text, retweet_count = public_metrics.retweet_count, 
            retweeted_id = referenced_tweets.retweeted.id
          )
      }
    ) %>% 
      bind_rows()
  ), 
  # Tweet number change plot. 
  tar_target(
    tw_num_plot, 
    csv %>% 
      # Bug: Do not have retweet information here. 
      mutate(retweet = case_when(
      is.na(retweeted_id) ~ "Non-retweet",
      TRUE ~ "Retweet"
    )) %>%
      group_by(date, retweet) %>%
      summarise(tw_num = n()) %>%
      ungroup() %>%
      mutate(
        retweet = factor(retweet, levels = c("Retweet", "Non-retweet"))
      ) %>%
      ggplot() + 
      geom_col(aes(date, tw_num)) + 
      labs(x = "Date", y = "Tweet number") + 
      scale_fill_discrete(name = "Tweet") + 
      theme(legend.justification = c(0, 1))
  ), 
  # Change of daily tweet number per 10 thousand users. 
  tar_target(
    general_plot_dt, 
    csv %>% 
      group_by(date) %>% 
      summarise(tw_num = n()) %>% 
      ungroup()
  ), 
  # Text analysis ----
  # User-defined dictionary. 
  tar_target(
    dict, 
    list(
      "地球 温暖 化", 
      "国立 環境 研究所", 
      "環境 研究所",
      "環境 研", 
      "江守 正多", 
      "江守 正多 先生", 
      "グレタ トゥーンベリ", 
      "地球 環境 研究 センター", 
      "再 稼働", 
      "強制 移住 防止 策", 
      "温暖 化", 
      "京都 議定書", 
      "松岡 修造", 
      "温室 効果 ガス", 
      "寒冷 化", 
      "砂漠 化", 
      "CO2", 
      "CO２", 
      "オゾン 層", 
      "可能 性", 
      "排出 量", 
      "議定 書", 
      "熱 中 症", 
      "科学 者", 
      "宇宙 ゴミ", 
      "数 年", 
      "パリ 協定", 
      "東京 タワー", 
      "小泉 進次郎", 
      "レジ 袋",
      "私 たち",
      "子供 たち",
      "生態 系",
      "花粉 症",
      "令 和",
      "化石 燃料",
      "気候 変動",
      "異常 気象",
      "気象 異常",
      "森林 伐採",
      "高齢 化",
      "子供 ごろ", 
      "子供 頃",
      "クマゼミ",
      "政治 家",
      "先進 国",
      "途上 国",
      "気温 上昇",
      "海面 上昇",
      "火力 発電",
      "氷 期",
      "具体 的",
      "人為 的",
      "十 年",
      "軍事 費",
      "人 たち",
      "自然 災害",
      "感染 症",
      "世界 的",
      "永久 凍土",
      "有料 化", 
      "一 番",
      "電気 代",
      "バイデン 氏",
      "COP",
      "科学 的",
      "陰謀 論", 
      "今 年", 
      "世界 の 財界",
      "今 月",
      "今 週",
      "今 日", 
      "麻生 氏", 
      "麻生 太郎", 
      "気候 変動"
    ) %>%  
      unique() %>% 
      setNames(1:length(.)) %>% 
      dictionary()
  ), 
  # Stop words. 
  tar_target(
    jp_stop_word, 
    tibble(
      word = c(
        stopwords("ja", source = "marimo"), 
        # Hiragana in Japanese: define Unicode code points for hiragana characters and convert code points to UTF-8 characters. 
        strsplit(intToUtf8(c(12353:12435)), "")[[1]], 
        "amp", "ます", "です", "こと", "って", "てい", "という", "んで", "ので", 
        "なく", "など", "なる", "せん", "しま", "とか", "しょう", "ろう", "けど", 
        "さん", "あっ", "られる", "ぜひ", "てる", "なら", "思い", "思う", "れる", 
        "たく", "なので", "ただ", "ほうが", "もの", "かも", "たら", "そう", " ",
        "いと", "とも", "どちら", "にし", "しく", "しか", "しな", "すぎ", "ほしい", 
        "おい", "なか"
      )
    )
  ),
  tar_target(
    # Stopwords for sentiment analysis, inherit from last part. . 
    stopword, jp_stop_word$word
  ), 
  # Make corpus. 
  tar_target(
    corp_2022, 
    csv %>% 
      filter(year == "2022", month <= 6) %>% 
      group_by(month) %>% 
      slice_sample(n = 1000) %>% 
      ungroup() %>% 
      corpus(., text_field = "text")
  ), 
  # Tokenization. 
  tar_target(
    tok_2022, 
    tokens(
      corp_2022, 
      remove_symbols = TRUE, 
      remove_numbers = TRUE, 
      remove_url = TRUE, 
      remove_separators = TRUE, 
      remove_punct = TRUE
    ) %>% 
      # Bug: Should delete text with just a few words? 
      tokens_compound(pattern = dict, concatenator = "") %>% 
      # Keep tokens in Japanese. 
      # Bug: Need to keep the words in other language? 
      tokens_select(
        pattern = "^[ぁ-んァ-ヶー一-龠]+$", valuetype = "regex", padding = TRUE
      ) %>% 
      tokens_remove(pattern = stopword)
  ), 
  # Document-term matrix. 
  tar_target(
    dtm_2022, 
    dfm(tok_2022)
  ), 
  # Document-term matrix in tm format. 
  tar_target(
    dtm_tm_2022, 
    quanteda::convert(dtm_2022, to = "tm")
  ), 
  # Target topic number. 
  tar_target(
    tar_k, 6
  ), 
  # LDA. 
  tar_target(
    lda_2022, 
    LDA(dtm_tm_2022, k = tar_k, control = list(seed = 1234))
  ), 
  # Topic words. 
  tar_target(
    topic_word_2022, 
    tidy(lda_2022, matrix = "beta") %>% 
      # Bug: Why there are empty term?
      filter(term != "") %>% 
      group_by(topic) %>% 
      slice_max(beta, n = 30) %>% 
      mutate(term = reorder_within(term, beta, topic)) %>% 
      summarise(term = list(term), .groups = "drop") %>% 
      mutate(
        term = unlist(lapply(term, function(x) paste0(x, collapse = ", ")))
      ) %>% 
      mutate(term = gsub("_", "", term), term = gsub("[0-9]", "", term))
  )
)
